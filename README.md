<div align="center">

# ğŸ§  Awesome Biomedical Foundation Models

This repository maintains an **up-to-date collection** of **foundation models** and **large-scale pre-training** research specifically tailored for **biomedical imaging**.
All papers are systematically organized by **publication year** to help researchers and clinicians navigate the rapidly evolving landscape of biomedical AI. If you find this curated gallery helpful for your research, feel free to give this repository a **star**! â­

<p align="center">
  <a href="https://github.com/qiangqwu/Awesome-Biomedical-Foundation-Models/pulls"><b>â¤¤ Submit a Pull Request</b></a> | 
  <a href="https://github.com/qiangqwu/Awesome-Biomedical-Foundation-Models"><b>â˜† Star the Repo</b></a>
</p>

---

<p align="center">
  <a href="#2026"><b>ğŸš€ 2026 Papers</b></a> |
  <a href="#2025"><b>ğŸŒŸ 2025 Papers</b></a> |
  <a href="#2024"><b>âœ¨ 2024 Papers</b></a> |
  <a href="#2023"><b>ğŸ’« 2023 Papers</b></a>
</p>

</div>

---

## ğŸ›ï¸ Gallery of Journal Papers

### 2026

> ### [Nature Medicine] A multimodal sleep foundation model for disease prediction
>
> **ğŸ‘¥ Authors:** *Rahul Thapa, Magnus Ruud Kjaer, Bryan He, Ian Covert, Hyatt Moore IV, Umaer Hanif, Gauri Ganjoo, M. Brandon Westover, Poul Jennum, Andreas Brink-Kjaer, Emmanuel Mignot, James Zou*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://www.nature.com/articles/s41591-025-04133-4)
>
> **ğŸ’¡ Summary:** A multimodal sleep foundation model trained with a new contrastive learning approach that accommodates multiple PSG configurations.
>
> **âš™ï¸ Computing Hardware:** Pretraining was performed on 432,000h of sleep data using **an NVIDIA A100 GPU**.

<br>

### 2025

> ### [Nature Medicine] A multimodal whole-slide foundation model for pathology
>
> **ğŸ‘¥ Authors:** *Tong Ding, Sophia J. Wagner, Andrew H. Song, Richard J. Chen, Ming Y. Lu, Andrew Zhang, Anurag J. Vaidya, Guillaume Jaume, Muhammad Shaban, Ahrong Kim, Drew F. K. Williamson, Harry Robertson, Bowen Chen, Cristina Almagro-PÃ©rez, Paul Doucet, Sharifa Sahai, Chengkuan Chen, Christina S. Chen, Daisuke Komura, Akihiro Kawabe, Mieko Ochi, Shinya Sato, Tomoyuki Yokose, Yohei Miyagi, Shumpei Ishikawa, Georg Gerber, Tingying Peng, Long Phi Le, Faisal Mahmood*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://www.nature.com/articles/s41591-025-03982-3)
>
> **ğŸ’¡ Summary:** A multimodal whole-slide foundation model pretrained using 335,645 whole-slide images via visual SSL and vision-language alignment.
>
> **âš™ï¸ Computing Hardware:** 8Ã— 80GB NVIDIA A100 GPUs (multi-node DDP).
<br>

> ### [Nature Methods] A visualâ€“omics foundation model to bridge histopathology with spatial transcriptomics
>
> **ğŸ‘¥ Authors:** *Weiqing Chen, Pengzhi Zhang, Tu N. Tran, Yiwei Xiao, Shengyu Li, Vrutant V. Shah, Hao Cheng, Kristopher W. Brannan, Keith Youker, Li Lai, Longhou Fang, Yu Yang, Nhat-Tu Le, Jun-ichi Abe, Shu-Hsia Chen, Qin Ma, Ken Chen, Qianqian Song, John P. Cooke, Guangyu Wang*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://www.nature.com/articles/s41592-025-02707-1)
>
> **ğŸ’¡ Summary:** OmiCLIP, a visualâ€“omics foundation model linking H&E images and transcriptomics using tissue patches from Visium data.
>
> **âš™ï¸ Computing Hardware:** Trained for 20 epochs using one **NVIDIA A100 80-GB GPU**.
<br>

> ### [Nature] Towards multimodal foundation models in molecular cell biology
>
> **ğŸ‘¥ Authors:** *Haotian Cui, Alejandro Tejada-Lapuerta, Maria BrbiÄ‡, Julio Saez-Rodriguez, Simona Cristea, Hani Goodarzi, Mohammad Lotfollahi, Fabian J. Theis, Bo Wang*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://www.nature.com/articles/s41586-025-08710-y)
>
> **ğŸ’¡ Summary:** Developing multimodal foundation models pretrained on diverse **omics** datasets, including genomics, transcriptomics, and spatial profiling.
<br>

> ### [Nature Medicine] Large language model-based biological age prediction in large-scale populations
>
> **ğŸ‘¥ Authors:** *Yanjun Li, Qi Huang, Jin Jiang, Xusheng Du, Wenxin Xiang, Shiqi Zhang, Zean Pan, Liyuan Zhao, Yuyan Cui, Limei Ke, Bo Yin, Linfeng Liu, Guoqing Feng, Shouyi Yan, Liangcai Gao, Yang Liu, Yujuan Yuan, Yanying Guo, Yuqing Yang, Weizhi Ma, Yining Yang, Qian Di*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://www.nature.com/articles/s41591-025-03856-8)
>
> **ğŸ’¡ Summary:** A framework leveraging LLMs to estimate individual overall and organ-specific aging using health examination reports.
<br>

> ### [Nature BME] A data-efficient strategy for building high-performing medical foundation models
>
> **ğŸ‘¥ Authors:** *Yuqi Sun, Weimin Tan, Zhuoyao Gu, Ruian He, Siyuan Chen, Miao Pang, Bo Yan*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://www.nature.com/articles/s41551-025-01365-0)
>
> **ğŸ’¡ Summary:** Synthetic data generated via conditioning with disease labels can be leveraged for building high-performing medical foundation models.
<br>

> ### [arXiv] Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations
>
> **ğŸ‘¥ Authors:** *Zhijian Yang, Noel DSouza, Istvan Megyeri, Xiaojian Xu, Amin Honarmandi Shandiz, Farzin Haddadpour, Krisztian Koos, Laszlo Rusko, Emanuele Valeriano, Bharadwaj Swaninathan, Lei Wu, Parminder Bhatia, Taha Kass-Hout, Erhan Bas*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://arxiv.org/abs/2509.21249)
>
> **ğŸ’¡ Summary:** MRI-specific vision-language foundation model for 3D representations.
<br>

### 2024

> ### [Nature Methods] A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities
>
> **ğŸ‘¥ Authors:** *Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Jacob Abel, Christine Moung-Wen, Brian Piening, Carlo Bifulco, Mu Wei, Hoifung Poon, Sheng Wang*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://www.nature.com/articles/s41592-024-02499-w)
>
> **ğŸ’¡ Summary:** BiomedParse, a foundation model that can jointly conduct segmentation, detection, and recognition across nine imaging modalities.
>
> **âš™ï¸ Computing Hardware:** 16Ã— NVIDIA A100-SXM4-40GB (58h training).
<br>

> ### [Nature Medicine] Towards a general-purpose foundation model for computational pathology
>
> **ğŸ‘¥ Authors:** *Richard J. Chen, Tong Ding, Ming Y. Lu, Drew F. K. Williamson, Guillaume Jaume, Andrew H. Song, Bowen Chen, Andrew Zhang, Daniel Shao, Muhammad Shaban, Mane Williams, Lukas Oldenburg, Luca L. Weishaupt, Judy J. Wang, Anurag Vaidya, Long Phi Le, Georg Gerber, Sharifa Sahai, Walt Williams, Faisal Mahmood*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://www.nature.com/articles/s41591-024-02857-3)
>
> **ğŸ’¡ Summary:** UNI, a general-purpose self-supervised model for pathology, pretrained using over **100 million images**.
>
> **âš™ï¸ Computing Hardware:** 4 Ã— 8 80GB NVIDIA A100 GPU nodes.
<br>

> ### [Nature Medicine] A visual-language foundation model for computational pathology
>
> **ğŸ‘¥ Authors:** *Ming Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, Long Phi Le, Georg Gerber, Anil V. Parwani, Andrew Zhang, Faisal Mahmood*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://www.nature.com/articles/s41591-024-02856-4)
>
> **ğŸ’¡ Summary:** CONCH, a visual-language foundation model developed using over 1.17 million histopathology imageâ€“caption pairs.
>
> **âš™ï¸ Computing Hardware:** 8Ã— NVIDIA A100 80-GB.
<br>

> ### [Nature Methods] scGPT: toward building a foundation model for single-cell multi-omics using generative AI
>
> **ğŸ‘¥ Authors:** *Haotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, Bo Wang*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://www.nature.com/articles/s41592-024-02201-0)
>
> **ğŸ’¡ Summary:** Foundation model for single-cell biology constructed via generative MAE pre-training.
<br>

> ### [NEJM AI] A Multimodal Biomedical Foundation Model Trained from Fifteen Million Imageâ€“Text Pairs
>
> **ğŸ‘¥ Authors:** *Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, Andrea Tupini, Yu Wang, Matt Mazzola, Swadheen Shukla, Lars Liden, Jianfeng Gao, Matthew P. Lungren, Tristan Naumann, Sheng Wang, Hoifung Poon*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://ai.nejm.org/doi/full/10.1056/AIoa2400640)
>
> **ğŸ’¡ Summary:** CLIP-Style Pre-training using the PCM-15M dataset.
<br>

> ### MedImageInsight: AN OPEN-SOURCE EMBEDDING MODEL FOR GENERAL DOMAIN MEDICAL IMAGING
>
> **ğŸ‘¥ Authors:** *Noel C. F. Codella, Ying Jin, Shrey Jain, Yu Gu, Ho Hin Lee, Asma Ben Abacha, Jenq-Neng Hwang, Thomas Lin, Ivan Tarapov, Matthew Lungren, Mu Wei*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://arxiv.org/abs/2410.06542)
>
> **ğŸ’¡ Summary:** Multi-modality CLIP training across diverse domains including X-Ray, CT, MRI, and Histopathology.
<br>

### 2023

> ### ğŸ”¬ [Biomedical Imaging] BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs
>
> **ğŸ‘¥ Authors:** *Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga*
>
> **ğŸ“„ Link:** [[Read the Paper]](https://arxiv.org/abs/2303.00915)
>
> **ğŸ’¡ Summary:** Proposing PMC-15M for text & biomedical image contrastive learning.
<br>

## Acknowledgement

Thanks for the fantastic works from the authors! Please feel free to a pull request for submitting new papers. Thank you for all your contributions.
<br>


